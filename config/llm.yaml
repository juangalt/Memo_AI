# LLM Provider Configuration and API Settings
# Memo AI Coach - LLM Configuration

provider:
  name: "claude"
  api_base_url: "https://api.anthropic.com"
  model: "claude-3-sonnet-20240229"
  api_version: "2023-06-01"

api_configuration:
  api_key: "${LLM_API_KEY}"
  timeout: 30
  max_retries: 3
  retry_delay: 1
  max_tokens: 4000
  temperature: 0.7
  top_p: 0.9
  frequency_penalty: 0.0
  presence_penalty: 0.0

request_settings:
  max_text_length: 10000
  min_text_length: 10
  chunk_size: 2000
  overlap_size: 200

response_handling:
  parse_json: true
  validate_response: true
  extract_structured_data: true
  handle_partial_responses: true
  retry_on_failure: true

error_handling:
  rate_limit_retry: true
  rate_limit_delay: 60
  max_rate_limit_retries: 3
  timeout_retry: true
  max_timeout_retries: 2
  fallback_response: true

debug_settings:
  log_requests: true
  log_responses: true
  log_timing: true
  log_errors: true
  store_raw_prompts: true
  store_raw_responses: true

performance_optimization:
  connection_pooling: true
  max_connections: 10
  connection_timeout: 10
  request_timeout: 30
  response_timeout: 30

monitoring:
  track_response_times: true
  track_error_rates: true
  track_token_usage: true
  track_cost_metrics: true
  alert_on_failures: true

fallback_configuration:
  enable_fallback: true
  fallback_provider: "openai"
  fallback_model: "gpt-3.5-turbo"
  fallback_api_key: "${FALLBACK_API_KEY}"
  fallback_conditions:
    - "rate_limit_exceeded"
    - "service_unavailable"
    - "timeout_error"
    - "authentication_error"

cost_management:
  track_costs: true
  cost_limit_per_request: 0.50
  cost_limit_per_day: 10.00
  cost_limit_per_month: 100.00
  alert_on_cost_limit: true
  auto_fallback_on_cost_limit: true

security:
  api_key_rotation: false
  key_rotation_interval: 90
  secure_transmission: true
  data_encryption: true
  audit_logging: true

model_specific_settings:
  claude:
    max_input_tokens: 200000
    max_output_tokens: 4000
    system_message_support: true
    function_calling: false
    vision_support: false
    
  openai:
    max_input_tokens: 4096
    max_output_tokens: 1000
    system_message_support: true
    function_calling: true
    vision_support: false
    
  gemini:
    max_input_tokens: 30720
    max_output_tokens: 2048
    system_message_support: false
    function_calling: false
    vision_support: true

environment_specific:
  development:
    debug_mode: true
    log_level: "DEBUG"
    mock_responses: true
    cost_tracking: false
    
  production:
    debug_mode: false
    log_level: "INFO"
    mock_responses: false
    cost_tracking: true
    fallback_enabled: true
